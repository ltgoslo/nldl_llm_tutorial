{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1060a3dd-88e2-4da5-a9b3-9b424d4bad4a",
   "metadata": {},
   "source": [
    "# LLM tutorial\n",
    "\n",
    "**Authors:**<br>\n",
    "*David Samuel, Egil Rønningstad, Andrey Kutuzov*<br>\n",
    "*University of Oslo, Language Technology Group*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7e140df2-d678-47cd-9cb8-2844f06e5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import clear_output, display_markdown\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52522247-43f0-48b6-83aa-4c2ea9925d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_NORWEGIAN_OKAY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988a331-0bb3-4bf6-9e51-11b65def802a",
   "metadata": {},
   "source": [
    "## 1. Load a pretrained generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a5f740-675b-4609-82f1-5c82850d78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'norallm/normistral-7b-warm'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\".\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=\".\"\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8742ca-0a35-4275-a3ea-f156bef39239",
   "metadata": {},
   "source": [
    "## 2. Make it generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ba549b8-9e50-44c7-ab38-8b6d36136262",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate(prefix: str, max_length=64, eos_token=\"\\n\", verbose=False) -> str:\n",
    "    if verbose: print(f\"PREFIX: {prefix}\")\n",
    "        \n",
    "    input_tokens = tokenizer(prefix, return_tensors='pt').input_ids.cuda()  # shape: [1, T]\n",
    "    if verbose: print(f\"INPUT: {str(input_tokens.shape)}\")\n",
    "\n",
    "    output_tokens = model.generate(\n",
    "        input_tokens,\n",
    "        max_new_tokens=max_length,\n",
    "        min_new_tokens=2,\n",
    "        eos_token_id=tokenizer(eos_token).input_ids if eos_token else None\n",
    "    )\n",
    "    if verbose: print(f\"OUTPUT: {str(output_tokens.shape)}\")\n",
    "\n",
    "    prediction = output_tokens[0, input_tokens.size(1):]\n",
    "    decoded_prediction = tokenizer.decode(prediction).strip()\n",
    "\n",
    "    return decoded_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31498800-d962-4d40-bb37-9387a87e5830",
   "metadata": {},
   "source": [
    "**TODO:** Try it with your own input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de8c94c9-c427-424b-a79e-8c1cde323b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: NLDL\n",
      "INPUT: torch.Size([1, 2])\n",
      "OUTPUT: torch.Size([1, 22])\n",
      "COMPLETION: NLDL), som er en organisasjon for alle som er interessert i og jobber med lundehund.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"NLDL\"  # TODO!\n",
    "\n",
    "response = generate(input_text, verbose=True, max_length=20, eos_token=None)\n",
    "print(f\"COMPLETION: {input_text}{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a518a98-ba95-4cdd-9a9e-f96b4ec2dfe7",
   "metadata": {},
   "source": [
    "## 3. Machine translation\n",
    "\n",
    "Can we turn the model into something more useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a202d42-499e-4475-bd07-260d05891408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for translation\n",
    "translation_prompt = \"\"\"{source_language}: {source_text}\n",
    "{target_language}:\"\"\"\n",
    "\n",
    "def translate(source_text: str, source_language=\"Bokmål\", target_language=\"Engelsk\", verbose=False) -> str:\n",
    "    text = translation_prompt.format(\n",
    "        source_text=source_text,\n",
    "        source_language=source_language,\n",
    "        target_language=target_language\n",
    "    )\n",
    "    return generate(text, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c36abcb4-5538-4e86-9660-d282ed7cb6d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: Bokmål: Du kan også prøve en annen tekst.\n",
      "Engelsk:\n",
      "INPUT: torch.Size([1, 15])\n",
      "OUTPUT: torch.Size([1, 23])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You can also try another text.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Du kan også prøve en annen tekst.\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57aece0-2b0c-492c-a3b5-45efeb7a6902",
   "metadata": {},
   "source": [
    "<br>We can use translation to translate Norwegian logs (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6a4a5ac-2101-4abe-9d2a-5c67ef9f57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(text: str, can_translate=True):\n",
    "    if not IS_NORWEGIAN_OKAY and can_translate:\n",
    "        print(f\"[no] {text}\")\n",
    "        print(f\"[en] {translate(text)}\\n\")\n",
    "    else:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aea3ec-4982-4c45-bfab-2609ddd45a47",
   "metadata": {},
   "source": [
    "## 4. Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00b64c62-f709-403f-b8e6-6f809ff47fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open('downsampled_data.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4950d731-2288-4938-90fb-b8f910562686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 0\n",
      "KEYS: ['id', 'question', 'choices', 'answer_label', 'answer_text']\n",
      "\n",
      "[no] Hva slags følelser skaper det å kjøpe gaver til andre?\n",
      "[en] What kind of emotions do you get from buying gifts for others?\n",
      "\n",
      "[no] Glede\n",
      "[en] Joy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "log(f\"EXAMPLE {index}\", can_translate=False)\n",
    "log(f\"KEYS: {str(list(dataset[index].keys()))}\\n\", can_translate=False)\n",
    "log(dataset[index][\"question\"])\n",
    "log(dataset[index][\"answer_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1ba02-258a-4dfb-92f9-5a830441abb2",
   "metadata": {},
   "source": [
    "### 4.1 Simple zero-shot generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10d23d4f-a5ec-472b-8a88-2d055132ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a better prompt template\n",
    "prompt = \"\"\"Spormål: {question}\n",
    "Svar:\"\"\"\n",
    "\n",
    "def zero_answer(sample: dict, verbose=False):\n",
    "    text = prompt.format(**sample)\n",
    "    return generate(text, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b940460d-5830-4db9-8b69-3754a4d8e730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: Spormål: Hvis du er sulten og går for å fiske, hva er hensikten med det?\n",
      "Svar:\n",
      "INPUT: torch.Size([1, 23])\n",
      "OUTPUT: torch.Size([1, 31])\n",
      "\n",
      "QUESTION:\n",
      "[no] Hvis du er sulten og går for å fiske, hva er hensikten med det?\n",
      "[en] If you are hungry and go fishing, what is the purpose of that?\n",
      "\n",
      "GOLD ANSWER:\n",
      "[no] Å få fisk\n",
      "[en] To get fish\n",
      "\n",
      "PREDICTION:\n",
      "[no] Det er for å få mat.\n",
      "[en] It's for getting food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "sample = dataset[index]\n",
    "\n",
    "prediction = zero_answer(sample, verbose=True)\n",
    "\n",
    "log(\"\\nQUESTION:\", can_translate=False)\n",
    "log(sample[\"question\"])\n",
    "\n",
    "log(\"GOLD ANSWER:\", can_translate=False)\n",
    "log(sample[\"answer_text\"])\n",
    "\n",
    "log(\"PREDICTION:\", can_translate=False)\n",
    "log(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16a65f-56e6-4116-8985-e69ef115efe2",
   "metadata": {},
   "source": [
    "When you're happy with the prompt, you can test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd901450-b142-45d2-b023-79e3d06b74b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:38<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_correct_predictions = 0\n",
    "for sample in tqdm(dataset):\n",
    "    prediction = zero_answer(sample)\n",
    "\n",
    "    if prediction.strip().lower() == sample[\"answer_text\"].strip().lower():\n",
    "        n_correct_predictions += 1\n",
    "\n",
    "log(f\"ACCURACY: {n_correct_predictions / len(dataset):.2%}\", can_translate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0fc3d-d365-4219-ba71-e5743113a3ed",
   "metadata": {},
   "source": [
    "### 4.2 Few-shot generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d5d0a60c-c616-4898-a012-9a4d8e0b9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_answer(dataset: List[dict], index: int, n_shots=2, verbose=False):\n",
    "    sample = dataset[index]\n",
    "\n",
    "    # select random demonstrations\n",
    "    other_samples = dataset[:index] + dataset[index + 1:]\n",
    "    shots = random.sample(other_samples, n_shots)\n",
    "    shot_prompts = [\n",
    "        f\"{prompt.format(**shot)} {shot['answer_text']}\"\n",
    "        for shot in shots\n",
    "    ]\n",
    "    input_text = '\\n\\n'.join(shot_prompts)\n",
    "    input_text = f\"{input_text}\\n\\n{prompt.format(**sample)}\"\n",
    "    \n",
    "    return generate(input_text, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "03ee435f-ed14-487b-b199-6e7e56877e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX: Spormål: Personen har tegnet innboforsikring. Hva er det han søker?\n",
      "Svar: Økonomisk trygghet\n",
      "\n",
      "Spormål: Hvor kan man finne frossen fisk på butikken?\n",
      "Svar: I frysedisken\n",
      "\n",
      "Spormål: Hvis du er sulten og går for å fiske, hva er hensikten med det?\n",
      "Svar:\n",
      "INPUT: torch.Size([1, 72])\n",
      "OUTPUT: torch.Size([1, 76])\n",
      "\n",
      "QUESTION:\n",
      "[no] Hvis du er sulten og går for å fiske, hva er hensikten med det?\n",
      "[en] If you are hungry and go fishing, what is the purpose of that?\n",
      "\n",
      "GOLD ANSWER:\n",
      "[no] Å få fisk\n",
      "[en] To get fish\n",
      "\n",
      "PREDICTION:\n",
      "[no] Å få fisk\n",
      "[en] To get fish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "\n",
    "prediction = few_answer(dataset, index, n_shots=2, verbose=True)\n",
    "\n",
    "log(\"\\nQUESTION:\", can_translate=False)\n",
    "log(sample[\"question\"])\n",
    "\n",
    "log(\"GOLD ANSWER:\", can_translate=False)\n",
    "log(sample[\"answer_text\"])\n",
    "\n",
    "log(\"PREDICTION:\", can_translate=False)\n",
    "log(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a65d4b65-80b5-414e-889e-797daddf3c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:13<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY for 1 shots: 37.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_SHOTS = 1\n",
    "\n",
    "n_correct_predictions = 0\n",
    "for index in tqdm(range(len(dataset))):\n",
    "    prediction = few_answer(dataset, index, N_SHOTS)\n",
    "\n",
    "    if prediction.strip().lower() == dataset[index][\"answer_text\"].strip().lower():\n",
    "        n_correct_predictions += 1\n",
    "\n",
    "log(f\"ACCURACY for {N_SHOTS} shots: {n_correct_predictions / len(dataset):.2%}\", can_translate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea108f2-b6d3-42f9-88a1-c0c949428beb",
   "metadata": {},
   "source": [
    "### 4.3 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "73d28b82-b693-45ac-878c-8df25d2835e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def classify(sample: dict, verbose=False):\n",
    "    question = sample[\"question\"]\n",
    "    options = sample[\"choices\"].values()\n",
    "\n",
    "    log_probabilities = {}\n",
    "    for option in options:\n",
    "        question_text = prompt.format(question=question)\n",
    "        question_ids = tokenizer(question_text, return_tensors='pt').input_ids.cuda()\n",
    "\n",
    "        option_text = f\" {option.lower()}\"\n",
    "        option_ids = tokenizer(option_text, return_tensors='pt', add_special_tokens=False).input_ids.cuda()\n",
    "\n",
    "        input_ids = torch.cat([question_ids, option_ids], dim=1)[:, :-1]\n",
    "        \n",
    "        output_logits = model(input_ids).logits\n",
    "        output_logits = output_logits[0, -option_ids.size(1):, :]\n",
    "        log_p = -torch.nn.functional.cross_entropy(output_logits, option_ids[0, :])\n",
    "\n",
    "        log_probabilities[option] = log_p\n",
    "\n",
    "    if verbose: print(log_probabilities)\n",
    "\n",
    "    prediction = max(log_probabilities, key=log_probabilities.get)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8e287710-2370-479f-b46a-33f0a28eea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Å se fisken': tensor(-5.1474, device='cuda:0'), 'Å ha det gøy': tensor(-3.3842, device='cuda:0'), 'Å få fisk': tensor(-3.0576, device='cuda:0'), 'Våte klær': tensor(-10.1656, device='cuda:0'), 'Å drepe': tensor(-5.8484, device='cuda:0')}\n",
      "\n",
      "QUESTION:\n",
      "[no] Hvis du er sulten og går for å fiske, hva er hensikten med det?\n",
      "[en] If you are hungry and go fishing, what is the purpose of that?\n",
      "\n",
      "GOLD ANSWER:\n",
      "[no] Å få fisk\n",
      "[en] To get fish\n",
      "\n",
      "PREDICTION:\n",
      "[no] Å få fisk\n",
      "[en] To get fish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "sample = dataset[index]\n",
    "\n",
    "prediction = classify(sample, verbose=True)\n",
    "\n",
    "log(\"\\nQUESTION:\", can_translate=False)\n",
    "log(sample[\"question\"])\n",
    "\n",
    "log(\"GOLD ANSWER:\", can_translate=False)\n",
    "log(sample[\"answer_text\"])\n",
    "\n",
    "log(\"PREDICTION:\", can_translate=False)\n",
    "log(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0fcfbf0e-2715-482f-b858-9be04ac7f88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:15<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 68.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_correct_predictions = 0\n",
    "for sample in tqdm(dataset):\n",
    "    prediction = classify(sample)\n",
    "\n",
    "    if prediction.strip().lower() == sample[\"answer_text\"].strip().lower():\n",
    "        n_correct_predictions += 1\n",
    "\n",
    "log(f\"ACCURACY: {n_correct_predictions / len(dataset):.2%}\", can_translate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c793e4-568e-4060-a004-aff78e9f8d48",
   "metadata": {},
   "source": [
    "### 4.4 Extra exercises\n",
    "\n",
    "Can we incorporate the A) B) C) D) E) answer option in the prompt?<br>\n",
    "Does it help to do classification togerther with a few-shot prompt?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
